{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yg6KMGlh3YqLGSy55_EAKSo3BBl4HxAT",
      "authorship_tag": "ABX9TyPWgmVUWRkJUrgmw7W90ACZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Velociraptorvelraptor/fake-food-generation-with-GAN-PyTorch-Lightning/blob/main/fake_food_GAN_PTL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeJos7nQQfCI"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning kaggle torchvision -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/kaggle.json /root/.kaggle"
      ],
      "metadata": {
        "id": "RIkfM-6EVMzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d trolukovich/food11-image-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGb9_9hFU1xf",
        "outputId": "092c8070-60c3-4ba2-a5b6-1b7579130adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading food11-image-dataset.zip to /content\n",
            " 99% 1.07G/1.08G [00:13<00:00, 147MB/s]\n",
            "100% 1.08G/1.08G [00:13<00:00, 86.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_path = '/content/drive/MyDrive/Colab Notebooks/GAN-fake-food'"
      ],
      "metadata": {
        "id": "EcZXRtD-VOOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv  '/content/food11-image-dataset.zip' '/content/drive/MyDrive/Colab Notebooks/GAN-fake-food'"
      ],
      "metadata": {
        "id": "7hCL1elygxo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/Colab Notebooks/GAN-fake-food/data'"
      ],
      "metadata": {
        "id": "20adhXcphhe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_path = src_path + '/data/training'"
      ],
      "metadata": {
        "id": "NC2HdKWHhSa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "EJBimxzNimOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_mtx = [(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)]"
      ],
      "metadata": {
        "id": "-61mBy8_pvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = T.Compose([\n",
        "    T.Resize(64),\n",
        "    T.CenterCrop(64),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*normalization_mtx)\n",
        "])"
      ],
      "metadata": {
        "id": "pWDf-eofjhRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_train_dataset = ImageFolder(training_path, transform=transformer)"
      ],
      "metadata": {
        "id": "Li6TI2UrjPGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128"
      ],
      "metadata": {
        "id": "YUlnUbCyn1L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(food_train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXjQKU_rkLcw",
        "outputId": "d8e9b158-b2b6-4ee9-bf33-af9edb5ef0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize(input_image_tensors):\n",
        "  input_image_tensors *= normalization_mtx[1][0]\n",
        "  input_image_tensors += normalization_mtx[0][0]  \n",
        "  return input_image_tensors"
      ],
      "metadata": {
        "id": "HV9JpeZUn6Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_samples(idx, sample_images):\n",
        "  \"\"\"\n",
        "  Takes epoch number and generated images and save last 64 (half of a batch size)\n",
        "  images in a grid 8x8.\n",
        "  idx: epoch number\n",
        "  sample_images: image returned by GAN model at the end of each epoch\n",
        "  \"\"\"\n",
        "  fake_fname = f'fake-img-{idx}.png'\n",
        "  save_image(denormalize(sample_images[-64:]), os.path.join(\".\", fake_fname), nrow=8)"
      ],
      "metadata": {
        "id": "_OsNPT1AqRcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FoodDiscriminator(nn.Module):\n",
        "  \"\"\"\n",
        "  Discriminator class which takes output from the generator model of size (3, 64, 64)\n",
        "  and generates output of either 0 (fake) or 1 (real).\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.channel = 3\n",
        "    self.kernel_size = 4\n",
        "    self.stride = 2\n",
        "    self.padding = 1\n",
        "    self.bias = False\n",
        "    self.negative_slope = 0.2\n",
        "\n",
        "    # (3, 64, 64)\n",
        "    self.conv1 = nn.Conv2d(self.channel, \n",
        "                           128, \n",
        "                           self.kernel_size, \n",
        "                           self.stride, \n",
        "                           self.padding, \n",
        "                           bias=self.bias)\n",
        "    self.bn1 = nn.BatchNorm2d(128)\n",
        "    self.relu = nn.LeakyReLU(self.negative_slope, inplace=True)\n",
        "\n",
        "    # (64, 32, 32)\n",
        "    self.conv2 = nn.Conv2d(128,\n",
        "                           256, \n",
        "                           self.kernel_size, \n",
        "                           self.stride, \n",
        "                           self.padding,\n",
        "                           bias=self.bias)\n",
        "    self.bn2 = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(256,\n",
        "                           512, \n",
        "                           self.kernel_size, \n",
        "                           self.stride, \n",
        "                           self.padding,\n",
        "                           bias=self.bias)\n",
        "    self.bn3 = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.conv4 = nn.Conv2d(512,\n",
        "                           1024, \n",
        "                           self.kernel_size, \n",
        "                           self.stride, \n",
        "                           self.padding,\n",
        "                           bias=self.bias)\n",
        "    self.bn4 = nn.BatchNorm2d(1024)\n",
        "\n",
        "    self.fc = nn.Sequential(nn.Linear(16384, 1),\n",
        "                            nn.Sigmoid())\n",
        "\n",
        "  def forward(self, input_img):\n",
        "    x = self.conv1(input_img)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.relu(x)\n",
        "    x = x.view(-1, 1024 * 4 * 4)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "65oJ8_etsIn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FoodGenerator(nn.Module):\n",
        "  \"\"\"\n",
        "  Generator class creates fake images by incorporating feedback from the discriminator.\n",
        "  latent_size: compressed low-dim represenation of the input (images)\n",
        "  \"\"\"\n",
        "  def __init__(self, latent_size=256):\n",
        "    super().__init__()\n",
        "    self.latent_size = latent_size\n",
        "    self.kernel_size = 4\n",
        "    self.stride = 2\n",
        "    self.padding = 1\n",
        "    self.bias = False\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        # input size: (latent_size, 1, 1)\n",
        "        nn.ConvTranspose2d(latent_size, 512, self.kernel_size, stride=1, padding=0, bias=self.bias),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # input size: (512, 4, 4)\n",
        "        nn.ConvTranspose2d(512, 256, self.kernel_size, self.stride, self.padding, bias=self.bias),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # input size: (256, 8, 8)\n",
        "        nn.ConvTranspose2d(256, 128, self.kernel_size, self.stride, self.padding, bias=self.bias),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # input size: (128, 16, 16)\n",
        "        nn.ConvTranspose2d(128, 64, self.kernel_size, self.stride, self.padding, bias=self.bias),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # input size: (64, 32, 32)\n",
        "        nn.ConvTranspose2d(64, 3, self.kernel_size, self.stride, self.padding, bias=self.bias),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, input_img):\n",
        "    return self.model(input_img)"
      ],
      "metadata": {
        "id": "Iyqy1d4H093F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FoodGAN():\n",
        "  def __init__(self, latent_size=256, learninig_rate=0.0002, bias1=0.5, bias2=0.999, batch_size=128):\n",
        "    super().__init__()\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "    self.generator = FoodGenerator()\n",
        "    self.discriminator = FoodDiscriminator(input_size=64)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.latent_size = latent_size\n",
        "    self.validation = torch.randn(self.batch_size, self.latent_size, 1, 1)\n",
        "\n",
        "    def adversial_loss(self, preds, targets):\n",
        "      return F.binary_cross_entropy(preds, targets)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      learning_rate = self.hparams.learninig_rate\n",
        "      bias1 = self.hparams.bias1\n",
        "      bias2 = self.hparams.bias2\n",
        "\n",
        "      opt_g = optim.Adam(self.generator.parameters(), lr=learninig_rate, betas=(bias1, bias2))\n",
        "      opt_d = optim.Adam(self.discriminator.parameters(), lr=learninig_rate, betas=(bias1, bias2))     \n",
        "\n",
        "      return [opt_g, opt_d], []\n",
        "\n",
        "    def forward(self, z):\n",
        "      return self.generator(z)\n",
        "\n",
        "    def train(self, batch, optimizer_idx):\n",
        "      real_img, _ = batch\n",
        "      if optimizer_idx == 0:\n",
        "        fake_random_noise = torch.randn(self.batch_size, self.latent_size, 1, 1)\n",
        "        fake_random_noise = fake_random_noise.type_as(real_img)\n",
        "        fake_img = self(fake_random_noise)\n",
        "        preds = self.discriminator(fake_img)\n",
        "        targets = torch.ones(self.batch_size, 1)\n",
        "        targets = targets.type_as(real_img)\n",
        "\n",
        "        loss = self.adversial_loss(preds, targets)\n",
        "        self.log('generator_loss', loss, prog_bar=True)\n",
        "\n",
        "        tqdm_dict = {'g_loss', loss}\n",
        "        output = OrderDict({\n",
        "            'loss': loss, \n",
        "            'progress_bar': tqdm_dict, \n",
        "            'log': tqdm_dict\n",
        "        })\n",
        "        return output"
      ],
      "metadata": {
        "id": "9_yOv0hMAaVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxa91lD1QNgr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}